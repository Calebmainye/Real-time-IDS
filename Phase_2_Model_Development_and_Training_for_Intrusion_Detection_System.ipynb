{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MMefYsSjRcK",
        "outputId": "b5e74a2a-f68c-45ad-d053-9fc63cfaab76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/preprocessed_data.zip\n",
            "replace /content/content/preprocessed_data/y_train.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: /content/content/preprocessed_data/y_train.npy  \n",
            "  inflating: /content/content/preprocessed_data/X_test.npy  \n",
            "  inflating: /content/content/preprocessed_data/selected_features.csv  \n",
            "  inflating: /content/content/preprocessed_data/y_test.npy  \n",
            "  inflating: /content/content/preprocessed_data/X_train.npy  \n",
            "  inflating: /content/content/preprocessed_data/scaler.pkl  \n"
          ]
        }
      ],
      "source": [
        "# prompt: Unzip this /content/preprocessed_data.zip\n",
        "\n",
        "!unzip /content/preprocessed_data.zip -d /content/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Model Development and Training for Intrusion Detection System\n",
        "##This notebook focuses on developing and training machine learning models using the **preprocessed CICIDS2017 dataset** to create an effective intrusion detection system\n",
        "# with reduced false positives."
      ],
      "metadata": {
        "id": "dr0RPJ6fvvNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import Required Libraries"
      ],
      "metadata": {
        "id": "meEOTJWnwno2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import joblib\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_curve, roc_auc_score,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "z8rWRYczwhge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load Preprocessed Data"
      ],
      "metadata": {
        "id": "NTbtQ_jHw4qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data(data_dir='/content/content/preprocessed_data'):\n",
        "    \"\"\"\n",
        "    Load the preprocessed data saved from Phase 1.\n",
        "\n",
        "    Parameters:\n",
        "    data_dir (str): Directory containing preprocessed data files\n",
        "\n",
        "    Returns:\n",
        "    tuple: (X_train, X_test, y_train, y_test, selected_features, scaler)\n",
        "    \"\"\"\n",
        "    print(\"Loading preprocessed data...\")\n",
        "\n",
        "    # Load training and testing data\n",
        "    X_train = np.load(f\"{data_dir}/X_train.npy\")\n",
        "    X_test = np.load(f\"{data_dir}/X_test.npy\")\n",
        "    y_train = np.load(f\"{data_dir}/y_train.npy\")\n",
        "    y_test = np.load(f\"{data_dir}/y_test.npy\")\n",
        "\n",
        "    # Load feature names\n",
        "    selected_features = pd.read_csv(f\"{data_dir}/selected_features.csv\").iloc[:, 0].tolist()\n",
        "\n",
        "    # Load scaler\n",
        "    scaler = joblib.load(f\"{data_dir}/scaler.pkl\")\n",
        "\n",
        "    print(f\"Loaded training data: {X_train.shape}\")\n",
        "    print(f\"Loaded testing data: {X_test.shape}\")\n",
        "    print(f\"Number of selected features: {len(selected_features)}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, selected_features, scaler\n",
        "\n",
        "# Load the preprocessed data\n",
        "X_train, X_test, y_train, y_test, selected_features, scaler = load_preprocessed_data()\n",
        "\n",
        "# Check if we're dealing with binary or multiclass classification\n",
        "n_classes = len(np.unique(y_train))\n",
        "problem_type = \"Binary Classification\" if n_classes == 2 else \"Multiclass Classification\"\n",
        "print(f\"\\nProblem type: {problem_type} with {n_classes} classes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKXdVYdPw6dW",
        "outputId": "b271c87a-ed49-4afd-c1be-bba6597d822a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed data...\n",
            "Loaded training data: (1818082, 30)\n",
            "Loaded testing data: (779179, 30)\n",
            "Number of selected features: 30\n",
            "\n",
            "Problem type: Binary Classification with 2 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define Model Evaluation Functions"
      ],
      "metadata": {
        "id": "1QQ3ZRXQyH8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, model_name, is_neural_network=False):\n",
        "    \"\"\"\n",
        "    Evaluate a trained model on the test set with a focus on false positive reduction.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained model\n",
        "    X_test: Test features\n",
        "    y_test: Test labels\n",
        "    model_name: Name of the model for display purposes\n",
        "    is_neural_network: Flag to indicate if model is a neural network\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    if is_neural_network:\n",
        "        y_prob = model.predict(X_test)\n",
        "        if y_prob.shape[1] > 1:  # Multiclass\n",
        "            y_pred = np.argmax(y_prob, axis=1)\n",
        "        else:  # Binary\n",
        "            y_pred = (y_prob > 0.5).astype('int').flatten()\n",
        "    else:\n",
        "        y_pred = model.predict(X_test)\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_prob = model.predict_proba(X_test)\n",
        "        else:\n",
        "            # For models that don't have predict_proba (like SVM without probability=True)\n",
        "            y_prob = None\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    if n_classes == 2:  # Binary classification\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        # Calculate false positive rate\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "        false_positive_rate = fp / (fp + tn)\n",
        "        false_negative_rate = fn / (fn + tp)\n",
        "\n",
        "        # ROC curve only for binary classification\n",
        "        if y_prob is not None:\n",
        "            if is_neural_network:\n",
        "                fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "                auc = roc_auc_score(y_test, y_prob)\n",
        "            else:\n",
        "                if y_prob.shape[1] > 1:\n",
        "                    fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
        "                    auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "                else:\n",
        "                    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "                    auc = roc_auc_score(y_test, y_prob)\n",
        "        else:\n",
        "            fpr, tpr, auc = None, None, None\n",
        "    else:  # Multiclass\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        # For multiclass, calculate per-class metrics\n",
        "        class_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "        # Calculate false positive and negative rates manually for multiclass\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        false_positive_rate = []\n",
        "        false_negative_rate = []\n",
        "        specificity = []\n",
        "\n",
        "        for i in range(n_classes):\n",
        "            fp = np.sum(cm[:, i]) - cm[i, i]\n",
        "            tn = np.sum(cm) - np.sum(cm[i, :]) - np.sum(cm[:, i]) + cm[i, i]\n",
        "            fn = np.sum(cm[i, :]) - cm[i, i]\n",
        "            tp = cm[i, i]\n",
        "\n",
        "            false_positive_rate.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
        "            false_negative_rate.append(fn / (fn + tp) if (fn + tp) > 0 else 0)\n",
        "            specificity.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
        "\n",
        "        # Average the rates\n",
        "        false_positive_rate = np.mean(false_positive_rate)\n",
        "        false_negative_rate = np.mean(false_negative_rate)\n",
        "        specificity = np.mean(specificity)\n",
        "\n",
        "        # No simple ROC curve for multiclass\n",
        "        fpr, tpr, auc = None, None, None\n",
        "\n",
        "    # Create confusion matrix plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig(f'confusion_matrix_{model_name.replace(\" \", \"_\").lower()}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Create ROC curve for binary classification\n",
        "    if n_classes == 2 and fpr is not None and tpr is not None:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'ROC Curve - {model_name}')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.savefig(f'roc_curve_{model_name.replace(\" \", \"_\").lower()}.png')\n",
        "        plt.close()\n",
        "\n",
        "    # Calculate execution time\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    # Compile and return results\n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'false_positive_rate': false_positive_rate,\n",
        "        'false_negative_rate': false_negative_rate,\n",
        "        'specificity': specificity,\n",
        "        'execution_time': execution_time\n",
        "    }\n",
        "\n",
        "    if auc is not None:\n",
        "        results['auc'] = auc\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nEvaluation Results for {model_name}:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
        "    print(f\"False Negative Rate: {false_negative_rate:.4f}\")\n",
        "    print(f\"Specificity: {specificity:.4f}\")\n",
        "    if auc is not None:\n",
        "        print(f\"AUC: {auc:.4f}\")\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def compare_models(results):\n",
        "    \"\"\"\n",
        "    Compare multiple models based on their evaluation metrics.\n",
        "\n",
        "    Parameters:\n",
        "    results (list): List of dictionaries containing model evaluation results\n",
        "    \"\"\"\n",
        "    # Extract key metrics for comparison\n",
        "    models = [r['model_name'] for r in results]\n",
        "    accuracy = [r['accuracy'] for r in results]\n",
        "    precision = [r['precision'] for r in results]\n",
        "    recall = [r['recall'] for r in results]\n",
        "    f1 = [r['f1_score'] for r in results]\n",
        "    fpr = [r['false_positive_rate'] for r in results]\n",
        "\n",
        "    # Create DataFrame for easy manipulation\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Model': models,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1,\n",
        "        'False Positive Rate': fpr\n",
        "    })\n",
        "\n",
        "    # Sort by false positive rate (ascending) and then by f1 score (descending)\n",
        "    comparison_df = comparison_df.sort_values(by=['False Positive Rate', 'F1 Score'], ascending=[True, False])\n",
        "\n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot metrics comparison\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "    df_plot = comparison_df.set_index('Model')[metrics]\n",
        "    df_plot.plot(kind='bar', figsize=(15, 6))\n",
        "    plt.title('Model Performance Comparison')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_performance_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot false positive rate specifically\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    sns.barplot(x='Model', y='False Positive Rate', data=comparison_df)\n",
        "    plt.title('False Positive Rate Comparison')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylabel('False Positive Rate')\n",
        "    plt.ylim(0, min(1.0, max(fpr) * 1.5))  # Adjust y-axis for better visualization\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('false_positive_rate_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Print comparison table\n",
        "    print(\"\\nModel Comparison Table (sorted by False Positive Rate):\")\n",
        "    comparison_df['Rank'] = range(1, len(comparison_df) + 1)\n",
        "    print(comparison_df.to_string(index=False, float_format='{:.4f}'.format))\n",
        "\n",
        "    return comparison_df"
      ],
      "metadata": {
        "id": "_XHDtBV-yK2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Define and Train Traditional Machine Learning Models"
      ],
      "metadata": {
        "id": "PljEvJyzzaZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_traditional_ml_models(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple traditional machine learning models.\n",
        "\n",
        "    Parameters:\n",
        "    X_train, y_train: Training data\n",
        "    X_test, y_test: Testing data\n",
        "\n",
        "    Returns:\n",
        "    list: Results for each model\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining traditional machine learning models...\")\n",
        "    results = []\n",
        "\n",
        "    # 1. Random Forest - good baseline for classification tasks\n",
        "    print(\"\\nTraining Random Forest...\")\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_results = evaluate_model(rf_model, X_test, y_test, \"Random Forest\")\n",
        "    results.append(rf_results)\n",
        "\n",
        "    # 2. Gradient Boosting - often performs well with imbalanced data\n",
        "    print(\"\\nTraining Gradient Boosting...\")\n",
        "    gb_model = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        random_state=42\n",
        "    )\n",
        "    gb_model.fit(X_train, y_train)\n",
        "    gb_results = evaluate_model(gb_model, X_test, y_test, \"Gradient Boosting\")\n",
        "    results.append(gb_results)\n",
        "\n",
        "    # 3. Support Vector Machine - good for high-dimensional spaces\n",
        "    print(\"\\nTraining SVM...\")\n",
        "    svm_model = SVC(\n",
        "        kernel='rbf',\n",
        "        C=1.0,\n",
        "        gamma='scale',\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    svm_results = evaluate_model(svm_model, X_test, y_test, \"SVM\")\n",
        "    results.append(svm_results)\n",
        "\n",
        "    # 4. KNN - simple but effective for some classification tasks\n",
        "    print(\"\\nTraining KNN...\")\n",
        "    knn_model = KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        weights='distance',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    knn_model.fit(X_train, y_train)\n",
        "    knn_results = evaluate_model(knn_model, X_test, y_test, \"KNN\")\n",
        "    results.append(knn_results)\n",
        "\n",
        "    # 5. Decision Tree - provides a baseline and feature importance\n",
        "    print(\"\\nTraining Decision Tree...\")\n",
        "    dt_model = DecisionTreeClassifier(\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42\n",
        "    )\n",
        "    dt_model.fit(X_train, y_train)\n",
        "    dt_results = evaluate_model(dt_model, X_test, y_test, \"Decision Tree\")\n",
        "    results.append(dt_results)\n",
        "\n",
        "    return results, rf_model, gb_model, svm_model  # Return best models for further optimization"
      ],
      "metadata": {
        "id": "qchbHWVuzqrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Define and Train Neural Network"
      ],
      "metadata": {
        "id": "NXtqhE6Lz-LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_neural_network(input_dim, output_dim):\n",
        "    \"\"\"\n",
        "    Create a neural network architecture for intrusion detection.\n",
        "\n",
        "    Parameters:\n",
        "    input_dim (int): Input dimension (number of features)\n",
        "    output_dim (int): Output dimension (number of classes)\n",
        "\n",
        "    Returns:\n",
        "    model: Compiled neural network model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Hidden layers\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Output layer\n",
        "    if output_dim == 2:  # Binary classification\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:  # Multiclass classification\n",
        "        model.add(Dense(output_dim, activation='softmax'))\n",
        "        loss = 'categorical_crossentropy'\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss=loss,\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_neural_network(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train a neural network for intrusion detection.\n",
        "\n",
        "    Parameters:\n",
        "    X_train, y_train: Training data\n",
        "    X_test, y_test: Testing data\n",
        "\n",
        "    Returns:\n",
        "    tuple: (results, model)\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining Neural Network...\")\n",
        "\n",
        "    # Prepare data for neural network\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    if n_classes == 2:  # Binary classification\n",
        "        output_dim = 2  # For architecture purposes\n",
        "        y_train_nn = y_train  # Keep as is for binary\n",
        "        y_test_nn = y_test\n",
        "    else:  # Multiclass classification\n",
        "        output_dim = n_classes\n",
        "        y_train_nn = to_categorical(y_train)\n",
        "        y_test_nn = to_categorical(y_test)\n",
        "\n",
        "    # Create and compile the model\n",
        "    nn_model = create_neural_network(input_dim, output_dim)\n",
        "\n",
        "    # Define callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Set up model checkpoint to save the best model\n",
        "    checkpoint_path = \"ids_neural_network.keras\"\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        checkpoint_path,\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = nn_model.fit(\n",
        "        X_train, y_train_nn if n_classes > 2 else y_train,\n",
        "        epochs=30,\n",
        "        batch_size=128,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping, model_checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('neural_network_training_history.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Evaluate the model\n",
        "    nn_results = evaluate_model(nn_model, X_test, y_test_nn if n_classes > 2 else y_test, \"Neural Network\", is_neural_network=True)\n",
        "\n",
        "    return nn_results, nn_model\n"
      ],
      "metadata": {
        "id": "QdC7p5Ti0A8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Optimize Best Performing Model"
      ],
      "metadata": {
        "id": "B0G950Ck1Ckg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_best_model(best_model_type, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Optimize hyperparameters for the best performing model to further reduce false positives.\n",
        "\n",
        "    Parameters:\n",
        "    best_model_type (str): Type of the best performing model\n",
        "    X_train, y_train: Training data\n",
        "    X_test, y_test: Testing data\n",
        "\n",
        "    Returns:\n",
        "    tuple: (optimized_model, results)\n",
        "    \"\"\"\n",
        "    print(f\"\\nOptimizing {best_model_type} model...\")\n",
        "\n",
        "    if best_model_type == \"Random Forest\":\n",
        "        # Define parameter grid for RandomForest\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [20, 30, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'class_weight': [None, 'balanced']\n",
        "        }\n",
        "\n",
        "        base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "    elif best_model_type == \"Gradient Boosting\":\n",
        "        # Define parameter grid for GradientBoosting\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'learning_rate': [0.05, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'subsample': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "\n",
        "        base_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "    elif best_model_type == \"SVM\":\n",
        "        # Define parameter grid for SVM\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'gamma': ['scale', 'auto', 0.1, 1],\n",
        "            'kernel': ['rbf', 'poly'],\n",
        "            'class_weight': [None, 'balanced']\n",
        "        }\n",
        "\n",
        "        base_model = SVC(probability=True, random_state=42)\n",
        "\n",
        "    else:\n",
        "        # If not one of the expected models, default to Random Forest\n",
        "        print(f\"Unexpected model type: {best_model_type}. Defaulting to Random Forest optimization.\")\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [20, 30, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'class_weight': [None, 'balanced']\n",
        "        }\n",
        "\n",
        "        base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Use RandomizedSearchCV to optimize hyperparameters\n",
        "    # This is faster than GridSearchCV with similar results when the parameter space is large\n",
        "    random_search = RandomizedSearchCV(\n",
        "        base_model,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=10,  # Number of parameter settings sampled\n",
        "        scoring='precision',  # Focus on precision to reduce false positives\n",
        "        cv=3,  # 3-fold cross-validation\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get best parameters and model\n",
        "    best_params = random_search.best_params_\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "    # Train a new model with the best parameters\n",
        "    optimized_model = random_search.best_estimator_\n",
        "\n",
        "    # Evaluate the optimized model\n",
        "    optimized_results = evaluate_model(optimized_model, X_test, y_test, f\"Optimized {best_model_type}\")\n",
        "\n",
        "    return optimized_model, optimized_results\n"
      ],
      "metadata": {
        "id": "2q9T9M4K1IS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Execute the Training and Evaluation Pipeline"
      ],
      "metadata": {
        "id": "Wij86tmp1RY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train traditional models\n",
        "traditional_results, rf_model, gb_model, svm_model = train_traditional_ml_models(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Train neural network\n",
        "nn_results, nn_model = train_neural_network(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Combine all results\n",
        "all_results = traditional_results + [nn_results]\n",
        "\n",
        "# Compare model performance\n",
        "comparison_df = compare_models(all_results)\n",
        "\n",
        "# Find best model based on false positive rate\n",
        "best_model_idx = comparison_df['False Positive Rate'].idxmin()\n",
        "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
        "\n",
        "print(f\"\\nBest model based on false positive rate: {best_model_name}\")\n",
        "\n",
        "# Map model name to model object\n",
        "model_mapping = {\n",
        "    \"Random Forest\": rf_model,\n",
        "    \"Gradient Boosting\": gb_model,\n",
        "    \"SVM\": svm_model,\n",
        "    \"Neural Network\": nn_model\n",
        "}\n",
        "\n",
        "# Determine best traditional model for optimization\n",
        "best_traditional_model_name = [m for m in model_mapping.keys() if m != \"Neural Network\" and m in comparison_df['Model'].values]\n",
        "best_traditional_model_name = comparison_df[comparison_df['Model'].isin(best_traditional_model_name)].sort_values('False Positive Rate').iloc[0]['Model']\n",
        "\n",
        "# Optimize the best traditional model\n",
        "if best_traditional_model_name in model_mapping:\n",
        "    optimized_model, optimized_results = optimize_best_model(\n",
        "        best_traditional_model_name,\n",
        "        X_train, y_train,\n",
        "        X_test, y_test\n",
        "    )\n",
        "\n",
        "    # Add optimized results to all results and update comparison\n",
        "    all_results.append(optimized_results)\n",
        "    final_comparison = compare_models(all_results)\n",
        "\n",
        "    # Final best model\n",
        "    final_best_model_idx = final_comparison['False Positive Rate'].idxmin()\n",
        "    final_best_model_name = final_comparison.loc[final_best_model_idx, 'Model']\n",
        "\n",
        "    print(f\"\\nFinal best model: {final_best_model_name}\")\n",
        "else:\n",
        "    print(f\"Could not optimize model: {best_traditional_model_name} not found in model mapping\")\n",
        "    final_best_model_name = best_model_name\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjdxGbJq1VZ6",
        "outputId": "0faa5f98-21e4-4d40-e1ac-c90816359c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training traditional machine learning models...\n",
            "\n",
            "Training Random Forest...\n",
            "\n",
            "Evaluation Results for Random Forest:\n",
            "Accuracy: 0.9977\n",
            "Precision: 0.9968\n",
            "Recall: 0.9978\n",
            "F1 Score: 0.9973\n",
            "False Positive Rate: 0.0024\n",
            "False Negative Rate: 0.0022\n",
            "Specificity: 0.9976\n",
            "AUC: 0.9999\n",
            "Execution Time: 11.06 seconds\n",
            "\n",
            "Training Gradient Boosting...\n",
            "\n",
            "Evaluation Results for Gradient Boosting:\n",
            "Accuracy: 0.9965\n",
            "Precision: 0.9948\n",
            "Recall: 0.9972\n",
            "F1 Score: 0.9960\n",
            "False Positive Rate: 0.0039\n",
            "False Negative Rate: 0.0028\n",
            "Specificity: 0.9961\n",
            "AUC: 0.9998\n",
            "Execution Time: 5.71 seconds\n",
            "\n",
            "Training SVM...\n"
          ]
        }
      ]
    }
  ]
}